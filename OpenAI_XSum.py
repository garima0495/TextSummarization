# -*- coding: utf-8 -*-
"""XSum_OpenAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ng7sk2XNQaP_a6phYN59Xreqq_Xe5Wh4
"""

!pip install openai

!pip install transformers
!pip install datasets
!pip install torch
!pip install peft

from datasets import load_dataset
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch.nn as nn
import torch


# Load the XSum dataset
data_files = {
    "train": "XsumTrain.csv",
    "test": "XsumTest.csv",
    "validation": "XsumValidation.csv"
}
dataset = load_dataset('csv', data_files=data_files)

# Load tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

def preprocess_function(examples):
    inputs = examples['document']
    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding="max_length")

    # Setup the tokenizer for targets
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(examples['summary'], max_length=128, truncation=True, padding="max_length")

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_datasets = dataset.map(preprocess_function, batched=True)

def apply_lora_and_peft(model, rank=32, lora_alpha=32, lora_dropout=0.1, adapter_size=64):
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            in_features = module.in_features
            out_features = module.out_features

            # LoRA
            lora_A = nn.Parameter(torch.Tensor(rank, in_features).uniform_(-0.1, 0.1))
            lora_B = nn.Parameter(torch.Tensor(out_features, rank).uniform_(-0.1, 0.1))
            module.lora_A = lora_A
            module.lora_B = lora_B

            original_forward = module.forward

            def lora_forward(input):
                return original_forward(input) + (module.lora_B @ module.lora_A @ input)

            module.forward = lora_forward

            # PEFT
            down = nn.Linear(in_features, adapter_size)
            up = nn.Linear(adapter_size, out_features)
            relu = nn.ReLU()

            original_forward = module.forward

            def adapter_forward(input):
                return original_forward(input) + up(relu(down(input)))

            module.forward = adapter_forward

apply_lora_and_peft(model)

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    num_train_epochs=3,
    weight_decay=0.01,
    save_total_limit=2,
    save_steps=10_000,
    fp16=True,
    gradient_accumulation_steps=8,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"]
)

# Train the model
trainer.train()

import time

def generate_summary_with_openai(article_text):
    prompt = f"Summarize the following article:\n\n{article_text}\n\nSummary:"

    response = openai.Completion.create(
        engine="text-davinci-003",  # or use "gpt-3.5-turbo" for more recent models
        prompt=prompt,
        max_tokens=100,  # Adjust the max tokens as needed
        temperature=0.1,  # Adjust the temperature for creativity
        top_p=1.0,
        frequency_penalty=0.0,
        presence_penalty=0.0
    )

    summary = response.choices[0].text.strip()
    return summary

# Create an empty column 'model_generated' in df to store the generated summaries
df['model_generated'] = ""

# Generate summaries and store them in the 'model_generated' column
for idx, row in df.iterrows():
    article_text = row['document']
    summary = generate_summary_with_openai(article_text)
    df.at[idx, 'model_generated'] = summary
    time.sleep(1)  # To avoid hitting API rate limits

# Display the first 25 generated summaries
print(df[['document', 'model_generated']].head(25))

import openai

# Replace 'your_api_key' with your actual API key
api_key = 'sk-proj-7c2PVoOmKXkGcSMMyZ9AT3BlbkFJN17FfVDMnrQQnQ7Bo8fP'
openai.api_key = api_key

!pip install datasets
from datasets import load_dataset

# Load the XSum dataset
dataset = load_dataset('xsum')

# Access the splits, e.g., 'test'
test_dataset = dataset['test']

# If you want to convert it to a pandas DataFrame, you can do so like this:
test_df = test_dataset.to_pandas()

test_df.head()

test_df.drop(columns=['id'], inplace=True)

test_df.head()

test_df['document'][0]

test_df['summary'][0]

!pip install openai==0.28

import time

import openai

def generate_summary_with_openai(article_text):
    # Define the prompt or question for generating the summary
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": f"Summarize this article:\n{article_text}"}
    ]

    # Generate text using the 'gpt-3.5-turbo' model via the chat completions API
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=messages,
        max_tokens=100,
        temperature=0.1
    )

    # Extract and return the generated summary
    summary = response['choices'][0]['message']['content']
    return summary.strip()

# Sample usage with hypothetical 'test_df' and 'document' column
test_df['model_generated'] = ""

batch_size = 3
delay_seconds = 60

for batch_start in range(0, 25, batch_size):
    batch_end = min(batch_start + batch_size, 25)
    articles_to_process = test_df['document'][batch_start:batch_end]

    generated_summaries = []
    for article_text in articles_to_process:
        summary = generate_summary_with_openai(article_text)
        generated_summaries.append(summary)

    test_df.loc[batch_start:batch_end-1, 'model_generated'] = generated_summaries

    if batch_end < 25:
        print(f"Generated summaries for articles {batch_start+1}-{batch_end}. Waiting for {delay_seconds} seconds before the next batch...")
        time.sleep(delay_seconds)

print(test_df[['document', 'model_generated']])

test_df.head(25)

test_df['model_generated'][11]

test_df['model_generated'][24]

for i in range(25):
    model_summary = test_df['model_generated'][i]
    reference_summary = test_df['summary'][i]
    print(f"{i + 1} - Reference Summary: {reference_summary}\nModel Summary: {model_summary}\n")

!pip install rouge

from rouge import Rouge

# Initialize the ROUGE evaluator
rouge = Rouge()

# Select the first 25 rows of your DataFrame for evaluation
num_samples = 25
sampled_df = test_df.head(num_samples)

# Extract the generated summaries and reference summaries for the selected samples
generated_summaries = sampled_df['model_generated'].tolist()
reference_summaries = sampled_df['summary'].tolist()

# Calculate ROUGE scores for the selected samples
rouge_scores = rouge.get_scores(generated_summaries, reference_summaries, avg=True)

# Print the ROUGE scores
print("ROUGE Scores:", rouge_scores)

from nltk.translate.bleu_score import corpus_bleu

# Select the first 25 rows of your DataFrame for evaluation
num_samples = 25
sampled_df = test_df.head(num_samples)

# Extract the generated summaries and reference summaries for the selected samples
generated_summaries = sampled_df['model_generated'].tolist()
reference_summaries = sampled_df['summary'].tolist()

# Calculate BLEU score for the selected samples
bleu_score = corpus_bleu(reference_summaries, generated_summaries)
print("BLEU Score for 25 Summaries:", bleu_score)

!pip install bert_score

from bert_score import score
# Select the first 25 rows of your DataFrame for evaluation
num_samples = 25
sampled_df = test_df.head(num_samples)

# Extract the generated summaries and reference summaries for the selected samples
generated_summaries = sampled_df['model_generated'].tolist()
reference_summaries = sampled_df['summary'].tolist()

# Calculate BERT Score
P, R, F1 = score(generated_summaries, reference_summaries, lang="en", verbose=True)

# Print BERT Score
print("BERT Precision:", P.mean().item())
print("BERT Recall:", R.mean().item())
print("BERT F1 Score:", F1.mean().item())